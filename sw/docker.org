* Docker
** Components
   Earlier it docker was a monolithic implementation but with the moby project it has been split into several components. Many of the components are community driven.
   The components are
   1. Docker CLI
   2. dockerd
   3. containerd
   4. runc
   5. containerd-ctr
   6. containerd-shim
*** Docker CLI
    It is the cli for docker. Docker cli communicates with the dockerd, which is the docker daemon using the api. All the logic is implemented inside the dockerd.
*** dockerd
    It is the docker daemon which exposes docker api. It manages container lifecycle by utilizing containerd. It can expose api in three sockets
    1. unix
    2. tcp
    3. fd
*** containerd
    Containerd is the industry standard container runtime. It is the container runtime that docker engine uses.
    containerd is the api facade for various runtimes and OS. This abstracts away the syscalls or OS specific functionality to run containers.
    It manages the complete container lifecycle of its hostsystem

    Components
      - Ctr
      - containerd-shim
      - RunC

    [[file:Containerd/chart-c_2018-06-12_22-17-14.png]]

    [[file:Containerd/chart-a_2018-06-12_22-16-28.png]]


    [[file:images/containerd.png]]



    Things it can do
      - containerd cli can be used to pull and push images
      - containerd cli can be used to run an image
      - You can plug in any container runtimes like runc

    Things it cannot do
      - containerd does not do build. Does not have docker composeS

**** runc
    runc is a cli for running applications packaged to the OCI format.

**** ctr
     It is a cli for containerd for debugging and development purpose.

**** shim
     This allows for daemonless containers. It is the parent process for all the containers. It helps to
     1. Allows runtimes (runc) to exit after start.
     2. Keeps stdio and other fds open, in case containerd / docker restarts. This way container can keep running even if docker restarts.
     3. This also helps to get exit status of containers without docker being parent for the containers.


*** Debugging
    Looking at the process history using ==ps fxa==
      #+BEGIN_QUOTE
      4141 ?        Ssl   41:36 /usr/bin/containerd
      4418 ?        Sl     0:52  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/15004a1a166df7e6e4060fce6d2f806d817a943a90572dd357c996f70a59e98b -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc
      4435 ?        Ss     0:20      \_ /sbin/tini -- /usr/local/bin/jenkins.sh
      4469 ?        Sl    93:39          \_ java -Duser.home=/var/jenkins_home -Djenkins.model.Jenkins.slaveAgentPort=50000 -jar /usr/share/jenkins/jenkins.war
      4142 ?        Ssl    6:04 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
      4410 ?        Sl     0:00  \_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 8090 -container-ip 172.17.0.2 -container-port 8080
      #+END_QUOTE

      containerd-shim is root for the jenkins container. It can survive containerd restart and docker restart.
* Ecosystem
** Open Container Initiative
   OCI has two specifications
   1. Runtime specifications
   2. Image specifications

* Networking
  - Three pillars of networking
    1. CNM
       - Container Network Model
       - Design for Docker Networking
       - Competing standard is CNI. This is for kubernetes.
       - Components
         1. Sandbox
            - Platform agnostic concept. In linux it is namespace
         2. Endpoint
            - This is network interface
         3. Network
            - Connected Endpoints
    2. Libnetwork
       - Real world implementation of CNM
       - Pluggable architecture
       - Control Plane and Management Plane
    3. Drivers
       - Different network types for libnetwork are implemented by drivers
       - Data Plane
       - Local Drivers are shipped by docker, Remote drivers are by 3rd party
  - Network Drivers
    1. bridge
    2. host
    3. none
    4. overlay
    5. macvlan
    6. ipvlan
  - Bridge Driver
    - Default driver
    - bridge network is the docker0 network which is present in all docker hosts. This is the default.
    - Single Host Networking
    - Useful for local development
    - External access to containers are done via port mapping
    - Bridge is also called vswitch / virtual switch
    - similar to default bridge, docker0, but with additional features
    - Also we can create docker_gwbridge and use it for communication with swarm nodes on different hosts
    - To create bridge network
      #+BEGIN_SRC bash
      docker network create -d bridge --subnet 10.0.0.1/24 ps-bridge
      #+END_SRC
  - overlay
    - Can span multiple hosts
    - Precondition to create overlay networks are
      1. Needs access to key value store. Supports consul, etcd, zookeeper
      2. cluster of hosts with connectivity to key value store
      3. properly configured engine daemon on each host on the swarm
    - Control plane is encrypted by default
    - Data plane can also be optionally encrypted
    - Overlay driver vxlan to build the overlay network
    - End of each VXLan Tunnel is the VTEP (VXlan tunnel Endpoint)
    - Can separate data plane and control plane into different network
  - MACVLAN
    - way to attach containers to existing networks and VLANs
    - Ideal for apps that are not ready to be fully containerized
    - Uses MACVLAN Network type
    - Each container gets a HW Mac address and ip address on the underlay network
    - Each container is visible on the underlay network
    - Requires Promiscuous mode
    - uses subinterfaces to process 802.1Q tags
  - Service Discovery
    - Service gets a Virtual IP (VIP) for loadbalancing
    - Behind the scenes it uses IPVS for transport layer loadbalancing
    - Name to IP Mapping are stored in Swarm KV Store
    - Container DNS and Docker Engine DNS are used to resolve DNS
    - Every container runs local DNS Resolver
    - Every Docker engine runs a DNS service
  - none network is used when the container does not need any network.
  - host network adds the container to the host network stack. You will find the container to have network identical to host
  - Default Network Drivers
    - bridge

* Docker Storage Drivers
** OverlayFS
   - Modern AUFS
   - Simple and Faster than AUFS
   - overlay driver uses OverlayFS to build and manage docker images and containers
   - There is two version of overlay driver. overlay and overlay2
   - Architecture
     [[file:images/overlayfs.jpg]]
   - Overlay driver works with single lower layer. So need hardlinks for implementation of multi layered images
   - Overlay2 driver natively works with multiple lower layer images.

* Best Practices
  1. Use SSD
  2. Use Data Volumes
* Docker Swarm
  - Components
    1. Swarm Manager
       - Filtering
       - Scheduling
    2. Discover services
       - Example: consul, zookeeper, etcd
  - Three scheduling stratergies
    1. Random
    2. Spread
    3. Binpack
       - Stopped containers are still considered for packing decision
* To Ponder
  - deis
  - funktion
  - parse
  - calico
    -
* Namespaces
** PID
** Mount
** Networks
** IPC
** UTS
** User
   - Root inside container is no longer a real system root
   - Ranges used by user and group are in
     /etc/subuid and /etc/subgid
   - Flag to enable in docker daemon is
     --userns-remap=default
* Commands
  capsh --print
  sudo capsh --drop=cap_chown,cap_setpcap,cap_setfcap,cap_sys_admin --chroot=$PWD/rootfs --
  sudo setcap cap_net_bind_service=+ep listen
  getcap listen
  mkdir /sys/fs/cgroup/memory/demo
  echo "100000000" > /sys/fs/cgroup/memory/demo/memory.limit_in_bytes
  echo "0" > /sys/fs/cgroup/memory/demo/memory.swappiness
  echo $$ > /sys/fs/cgroup/memory/demo/tasks
  sudo mount --bind -o ro $PWD/readonlyfiles $PWD/rootfs/var/readonlyfiles
  sudo nsenter --pid=/proc/29840/ns/pid \
    unshare -f --mount-proc=$PWD/rootfs/proc \
    chroot rootfs /bin/bash
  sudo unshare -p -f --mount-proc=$PWD/rootfs/proc \
    chroot rootfs /bin/bash

    #+BEGIN_SRC bash
    pred='process matches ".*(ocker|vpnkit).*" || (process in {"taskgated-helper", "launchservicesd", "kernel"} && eventMessage contains[c] "docker")'
    /usr/bin/log stream --style syslog --level=debug --color=always --predicate "$pred"
    #+END_SRC
* Articles
  https://ericchiang.github.io/post/containers-from-scratch/

* Tools
** Buildkit
   [[https://github.com/genuinetools/img][img]]
   [[https://github.com/moby/buildkit][buildkit]]
* Docker for windows
  - There is an option for isolating containers using hyperv
  - Windows team implemented Compute services which runs and exposes apis for docker to talk to
  - Containers run on container user mode which is different from user mode for windows.
  - Each container will also have additional processes along with the one being started to support containerization.
  -
* Networking
  - All containers have unique ip address in a cluster and they should be able to talk to each other without NAT
  - All nodes can communicate with containers without NAT
  - Commands
    https://www.youtube.com/watch?v=6v_BDHIgOY8
    1. Create a namespace
       sudo ip netns add $CON
    2. Create a veth pair
       sudo ip link add veth1 type veth peer name veth2
    3. Set one end of the pair to the namespace
       sudo ip link set veth2 netns $CON
    4. Configure with ip address
       sudo ip netns exec $CON ip addr add $IP dev veth2
    5. Enable the interface to be up
       sudo ip netns exec $CON ip link set dev veth2 up
    6. Enable the interface on the node
       sudo ip link set dev veth1 up
    7. Set the loopback interface on container
       sudo ip netns exec $CON ip link set lo up
    8. Set route on the node
       sudo ip route add $IP/32 dev veth1
    9. Set route on the container
       sudo ip netns exec $CON ip route add default via $IP dev veth2
    10. Create a bridge
        sudo ip link add name br0 type bridge
    11. Add veth to bridge
        sudo ip link set dev veth10 master br0
    12. Add ip address to bridge
        sudo ip addr add $BRIDGE_IP/24 dev br0
    13. Enable the bridge
        sudo ip link set dev br0 up
    14.
