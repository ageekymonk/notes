* Container Ecosystem
** Open Container Initiative
   OCI has two specifications
   1. Runtime specifications
   2. Image specifications

* Namespaces
  Linux namespaces along with cgroups are the core feature of linux kernel that enable the containers possible.
** PID
** Mount
** Networks
*** Debugging
    #+BEGIN_SRC
docker inspect --format '{{.State.Pid}}' <container_name_or_Id>
nsenter -t <PID> -n ip addr show

pid = "$(docker inspect -f '{{.State.Pid}}' "container_name | Uuid")"
mkdir -p /var/run/netns
ln -sf /proc/$pid/ns/net "/var/run/netns/container_name or uuid"
ip netns
ip netns exec "container name | uuid" ip a
    #+END_SRC
** IPC
** UTS
** User
   - Root inside container is no longer a real system root
   - Ranges used by user and group are in
     /etc/subuid and /etc/subgid
   - Flag to enable in docker daemon is
     --userns-remap=default
* Networking
  - Three pillars of networking
    1. CNM
       - Container Network Model
       - Design for Docker Networking
       - Competing standard is CNI. This is for kubernetes.
       - Components
         1. Sandbox
            - Platform agnostic concept. In linux it is namespace
         2. Endpoint
            - This is network interface
         3. Network
            - Connected Endpoints
    2. Libnetwork
       - Real world implementation of CNM
       - Pluggable architecture
       - Control Plane and Management Plane
    3. Drivers
       - Different network types for libnetwork are implemented by drivers
       - Data Plane
       - Local Drivers are shipped by docker, Remote drivers are by 3rd party
  - Network Drivers
    1. bridge
    2. host
    3. none
    4. overlay
    5. macvlan
    6. ipvlan
  - Bridge Driver
    - Default driver
    - bridge network is the docker0 network which is present in all docker hosts. This is the default.
    - Single Host Networking
    - Useful for local development
    - External access to containers are done via port mapping
    - Bridge is also called vswitch / virtual switch
    - similar to default bridge, docker0, but with additional features
    - Also we can create docker_gwbridge and use it for communication with swarm nodes on different hosts
    - To create bridge network
      #+BEGIN_SRC bash
      docker network create -d bridge --subnet 10.0.0.1/24 ps-bridge
      #+END_SRC
  - overlay
    - Can span multiple hosts
    - Precondition to create overlay networks are
      1. Needs access to key value store. Supports consul, etcd, zookeeper
      2. cluster of hosts with connectivity to key value store
      3. properly configured engine daemon on each host on the swarm
    - Control plane is encrypted by default
    - Data plane can also be optionally encrypted
    - Overlay driver vxlan to build the overlay network
    - End of each VXLan Tunnel is the VTEP (VXlan tunnel Endpoint)
    - Can separate data plane and control plane into different network
  - MACVLAN
    - way to attach containers to existing networks and VLANs
    - Ideal for apps that are not ready to be fully containerized
    - Uses MACVLAN Network type
    - Each container gets a HW Mac address and ip address on the underlay network
    - Each container is visible on the underlay network
    - Requires Promiscuous mode
    - uses subinterfaces to process 802.1Q tags
  - Service Discovery
    - Service gets a Virtual IP (VIP) for loadbalancing
    - Behind the scenes it uses IPVS for transport layer loadbalancing
    - Name to IP Mapping are stored in Swarm KV Store
    - Container DNS and Docker Engine DNS are used to resolve DNS
    - Every container runs local DNS Resolver
    - Every Docker engine runs a DNS service
  - none network is used when the container does not need any network.
  - host network adds the container to the host network stack. You will find the container to have network identical to host
  - Default Network Drivers
    - bridge

* Docker Storage Drivers
** OverlayFS
   - Modern AUFS
   - Simple and Faster than AUFS
   - overlay driver uses OverlayFS to build and manage docker images and containers
   - There is two version of overlay driver. overlay and overlay2
   - Architecture
     [[file:images/overlayfs.jpg]]
   - Overlay driver works with single lower layer. So need hardlinks for implementation of multi layered images
   - Overlay2 driver natively works with multiple lower layer images.

* Best Practices
  1. Use SSD
  2. Use Data Volumes
* Docker Swarm
  - Components
    1. Swarm Manager
       - Filtering
       - Scheduling
    2. Discover services
       - Example: consul, zookeeper, etcd
  - Three scheduling stratergies
    1. Random
    2. Spread
    3. Binpack
       - Stopped containers are still considered for packing decision
* To Ponder
  - deis
  - funktion
  - parse
  - calico
    -
* Commands
  capsh --print
  sudo capsh --drop=cap_chown,cap_setpcap,cap_setfcap,cap_sys_admin --chroot=$PWD/rootfs --
  sudo setcap cap_net_bind_service=+ep listen
  getcap listen
  mkdir /sys/fs/cgroup/memory/demo
  echo "100000000" > /sys/fs/cgroup/memory/demo/memory.limit_in_bytes
  echo "0" > /sys/fs/cgroup/memory/demo/memory.swappiness
  echo $$ > /sys/fs/cgroup/memory/demo/tasks
  sudo mount --bind -o ro $PWD/readonlyfiles $PWD/rootfs/var/readonlyfiles
  sudo nsenter --pid=/proc/29840/ns/pid \
    unshare -f --mount-proc=$PWD/rootfs/proc \
    chroot rootfs /bin/bash
  sudo unshare -p -f --mount-proc=$PWD/rootfs/proc \
    chroot rootfs /bin/bash

    #+BEGIN_SRC bash
    pred='process matches ".*(ocker|vpnkit).*" || (process in {"taskgated-helper", "launchservicesd", "kernel"} && eventMessage contains[c] "docker")'
    /usr/bin/log stream --style syslog --level=debug --color=always --predicate "$pred"
    #+END_SRC
* Articles
  https://ericchiang.github.io/post/containers-from-scratch/

* Tools
** Buildkit
   [[https://github.com/genuinetools/img][img]]
   [[https://github.com/moby/buildkit][buildkit]]
* Docker for windows
  - There is an option for isolating containers using hyperv
  - Windows team implemented Compute services which runs and exposes apis for docker to talk to
  - Containers run on container user mode which is different from user mode for windows.
  - Each container will also have additional processes along with the one being started to support containerization.
  -
* Networking
  - All containers have unique ip address in a cluster and they should be able to talk to each other without NAT
  - All nodes can communicate with containers without NAT
  - Commands
    https://www.youtube.com/watch?v=6v_BDHIgOY8
    1. Create a namespace
       sudo ip netns add $CON
    2. Create a veth pair
       sudo ip link add veth1 type veth peer name veth2
    3. Set one end of the pair to the namespace
       sudo ip link set veth2 netns $CON
    4. Configure with ip address
       sudo ip netns exec $CON ip addr add $IP dev veth2
    5. Enable the interface to be up
       sudo ip netns exec $CON ip link set dev veth2 up
    6. Enable the interface on the node
       sudo ip link set dev veth1 up
    7. Set the loopback interface on container
       sudo ip netns exec $CON ip link set lo up
    8. Set route on the node
       sudo ip route add $IP/32 dev veth1
    9. Set route on the container
       sudo ip netns exec $CON ip route add default via $IP dev veth2
    10. Create a bridge
        sudo ip link add name br0 type bridge
    11. Add veth to bridge
        sudo ip link set dev veth10 master br0
    12. Add ip address to bridge
        sudo ip addr add $BRIDGE_IP/24 dev br0
    13. Enable the bridge
        sudo ip link set dev br0 up
    14.
